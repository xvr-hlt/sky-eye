{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/xvr-hlt/sky-eye-full\" target=\"_blank\">https://app.wandb.ai/xvr-hlt/sky-eye-full</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/xvr-hlt/sky-eye-full/runs/ch8ist1l\" target=\"_blank\">https://app.wandb.ai/xvr-hlt/sky-eye-full/runs/ch8ist1l</a><br/>\n",
       "                Docs: <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">https://docs.wandb.com/integrations/jupyter.html</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "wandb.init(project=\"sky-eye-full\")\n",
    "conf = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "train_dir = '/home/jupyter/datasets/xview/train'\n",
    "test_dir = '/home/jupyter/datasets/xview/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from xv.util import vis_im_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.aug_prob = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as al\n",
    "\n",
    "augment = al.Compose([\n",
    "        al.HorizontalFlip(p=conf.aug_prob),\n",
    "        al.VerticalFlip(p=conf.aug_prob),\n",
    "        al.RandomRotate90(p=conf.aug_prob),\n",
    "        al.Transpose(p=conf.aug_prob),\n",
    "        al.GridDistortion(p=conf.aug_prob, distort_limit=.2),\n",
    "        al.ShiftScaleRotate(p=conf.aug_prob),\n",
    "        al.RandomBrightnessContrast(p=conf.aug_prob)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.n_dmg_classes = 4\n",
    "conf.batch_size = 6\n",
    "conf.image_size = 512\n",
    "\n",
    "conf.damage_scale_mode = 'ordinal'\n",
    "\n",
    "conf.blocktype = 'bottleneck'\n",
    "\n",
    "conf.blocks = [2, 4]\n",
    "conf.strides = [2, 2]\n",
    "conf.growth_rate = 1\n",
    "\n",
    "dmg_downscale=1\n",
    "for s in conf.strides:\n",
    "    dmg_downscale *= s\n",
    "\n",
    "conf.dmg_downscale_ratio = dmg_downscale\n",
    "\n",
    "conf.pretrained_model = 'selimsef_spacenet4_densenet121unet'\n",
    "conf.pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/sky-eye/xv/nn/solaris/zoo/selim_sef_sn4.py:472: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "from xv.nn.solaris.model_io import get_model\n",
    "building_seg = get_model(conf.pretrained_model, 'torch', pretrained=conf.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "block_types = {\n",
    "    'bottleneck': Bottleneck,\n",
    "    'basic': BasicBlock\n",
    "}\n",
    "\n",
    "class DamageHeatmap(nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, blocks, strides, block, nclasses, growth_rate=2):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        features = []\n",
    "        planes = inplanes\n",
    "        for stride, nblock in zip(strides, blocks):\n",
    "            planes *= growth_rate\n",
    "            features.append(self._make_layer(inplanes, planes, stride, nblock))\n",
    "            inplanes = planes * block.expansion\n",
    "            \n",
    "        self.features = nn.ModuleList(features)\n",
    "        self.head = nn.Conv2d(inplanes, nclasses, kernel_size=1, padding=0)\n",
    "        self._init_weights()\n",
    "\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Bottleneck):\n",
    "                nn.init.constant_(m.bn3.weight, 0)\n",
    "            elif isinstance(m, BasicBlock):\n",
    "                nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, inplanes, planes, stride, nblocks):\n",
    "        if stride != 1 or inplanes != planes * self.block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * self.block.expansion, kernel_size=1, stride=stride, bias=False), \n",
    "                nn.BatchNorm2d(planes * self.block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(self.block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * self.block.expansion\n",
    "        for _ in range(1, nblocks):\n",
    "            layers.append(self.block(inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for feature in self.features:\n",
    "            x = feature(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage = DamageHeatmap(inplanes=building_seg.final_filters, blocks=conf.blocks, strides=conf.strides,\n",
    "                       block=block_types[conf.blocktype],\n",
    "                       nclasses=conf.n_dmg_classes,\n",
    "                       growth_rate=conf.growth_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XVNet(nn.Module):\n",
    "    def __init__(self, building_seg, dmg_heatmap):\n",
    "        super().__init__()\n",
    "        self.building_seg = building_seg\n",
    "        self.dmg_heatmap = dmg_heatmap\n",
    "    \n",
    "    def forward(self, pre, post):\n",
    "        pre = self.building_seg(pre)\n",
    "        post = self.building_seg(post, apply_head=False)\n",
    "        post = self.dmg_heatmap(post)\n",
    "        return pre, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XVNet(building_seg, damage).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fa3b9724524a6e827ad9568d6350f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2799), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from xv.nn import dataset\n",
    "from xv import util\n",
    "import random\n",
    "\n",
    "instances = dataset.XViewSegmentationDataset.get_instances(train_dir)\n",
    "\n",
    "random.seed(hash(\"ðŸ˜‚\"))\n",
    "random.shuffle(instances)\n",
    "\n",
    "dev_ix = int(len(instances)*.20)\n",
    "dev_instances = instances[:dev_ix]\n",
    "train_instances = instances[dev_ix:]\n",
    "len(train_instances), len(dev_instances)\n",
    "\n",
    "train_dataset = dataset.XViewSegmentationDataset(\n",
    "    instances=train_instances,\n",
    "    resolution=(conf.image_size, conf.image_size),\n",
    "    dmg_downscale_ratio = conf.dmg_downscale_ratio,\n",
    "    augment=augment,\n",
    "    damage_scale_mode=conf.damage_scale_mode\n",
    ")\n",
    "dev_dataset = dataset.XViewSegmentationDataset(\n",
    "    instances=dev_instances,\n",
    "    resolution=(conf.image_size, conf.image_size),\n",
    "    dmg_downscale_ratio = conf.dmg_downscale_ratio,\n",
    "    augment=None,\n",
    "    damage_scale_mode=conf.damage_scale_mode\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn.losses import loss_dict, WeightedLoss\n",
    "\n",
    "conf.loss_weights = {\n",
    "    'dice': 1,\n",
    "    'focal': 1,\n",
    "    #'bcewithlogits': 1,\n",
    "    #'jaccard': 1\n",
    "}\n",
    "\n",
    "loss = WeightedLoss({loss_dict[l]():w for l, w in conf.loss_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex\n",
    "\n",
    "optims = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adamw': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "conf.optim = 'adam'\n",
    "conf.lr = 0.0005\n",
    "\n",
    "optim = optims[conf.optim](model.parameters(), lr=conf.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "conf.amp_opt_level = 'O1'\n",
    "model, optim = amp.initialize(model, optim, opt_level=conf.amp_opt_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.scheduler_factor = 0.5\n",
    "conf.scheduler_patience = 5\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=conf.scheduler_factor, patience=conf.scheduler_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.pre_weight = 1.\n",
    "conf.post_weight = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def train(model, optim, data, loss_fn):\n",
    "    model = model.train()\n",
    "    loss_sum, loss_pre_sum, loss_post_sum = 0., 0., 0.\n",
    "    \n",
    "    for batch in tqdm(iter(data)):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        pre, post = batch['images']['image'].to('cuda'), batch['images']['post'].to('cuda')\n",
    "        pre_out, post_out = model(pre, post)\n",
    "        \n",
    "        loss_pre = conf.pre_weight*loss_fn(pre_out, batch['masks']['buildings'].to('cuda'))\n",
    "        loss_post = conf.post_weight*sum((loss_fn(mask_out, mask) for mask_out, mask in zip(post_out, batch['masks']['damage'].to('cuda'))))\n",
    "        loss_post /= post_out.shape[1]\n",
    "        \n",
    "        loss = (loss_pre + loss_post)/(conf.pre_weight+conf.post_weight)\n",
    "        \n",
    "        with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_sum += loss\n",
    "        loss_pre_sum += loss_pre\n",
    "        loss_post_sum += loss_post\n",
    "        \n",
    "    return {\n",
    "        'loss':loss_sum/len(data), \n",
    "        'loss_pre': loss_pre_sum/len(data),\n",
    "        'loss_post': loss_post_sum/len(data)\n",
    "    }\n",
    "\n",
    "def batch_metrics(outputs, targets, threshold=0.5):\n",
    "    metrics = {}\n",
    "    pr_sum, re_sum, f_sum = 0., 0., 0.\n",
    "    for output, target in zip(outputs, targets):\n",
    "        target_bool = target.bool()\n",
    "        output_bool = output.sigmoid() > threshold\n",
    "\n",
    "        recall = output_bool[target_bool].float().mean()\n",
    "        recall = recall if recall == recall else 1.\n",
    "\n",
    "        precision = target_bool[output_bool].float().mean()\n",
    "        precision = precision if precision == precision else 1.\n",
    "\n",
    "        pr_sum += precision\n",
    "        re_sum += recall\n",
    "        f_sum += 2*precision*recall/(precision + recall) if (precision + recall) > 0. else 0.\n",
    "\n",
    "    return {\n",
    "        'recall': re_sum/len(outputs),\n",
    "        'precision': pr_sum/len(outputs),\n",
    "        'f1': f_sum/len(outputs)\n",
    "    }\n",
    "    \n",
    "\n",
    "def evaluate(model, optim, data, loss_fn, threshold=0.5):\n",
    "    model = model.eval()\n",
    "    metrics = defaultdict(float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        metric_sums = defaultdict(float)\n",
    "        for batch in tqdm(iter(data)):\n",
    "            pre, post = batch['images']['image'].to('cuda'), batch['images']['post'].to('cuda')\n",
    "            pre_out, post_out = model(pre, post)\n",
    "            pre_targets, post_targets = batch['masks']['buildings'].to('cuda'), batch['masks']['damage'].to('cuda')\n",
    "            \n",
    "            loss_pre = conf.pre_weight*loss_fn(pre_out, pre_targets)\n",
    "            loss_post = conf.post_weight*sum((loss_fn(mask_out, mask) for mask_out, mask in zip(post_out, post_targets)))\n",
    "            loss = (loss_pre + loss_post)/(conf.pre_weight+conf.post_weight)\n",
    "            \n",
    "            metrics['loss_pre'] += loss_pre\n",
    "            metrics['loss_post'] += loss_post\n",
    "            metrics['loss'] += loss\n",
    "            \n",
    "            macro_metrics = defaultdict(float)\n",
    "            for dmg_type, ix in train_dataset.DAMAGE_CLASSES.items():\n",
    "                for k,v in batch_metrics(post_out[:,ix], post_targets[:,ix]).items():\n",
    "                    metrics[f'dmg_{dmg_type}_{k}'] += v\n",
    "                    macro_metrics[k] += v\n",
    "            for k, v in macro_metrics.items():\n",
    "                metrics[f'dmg_macro_{k}'] += v/len(train_dataset.DAMAGE_CLASSES)\n",
    "            for k,v in batch_metrics(pre_out, pre_targets).items():    \n",
    "                metrics[f'building_{k}'] += v\n",
    "    return {k:v/len(data) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.epochs = 140\n",
    "best_loss = 1e5\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6780fc29ae4725bb104352305162b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=374), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, conf.epochs):\n",
    "    metrics = {}\n",
    "    metrics.update(({f'train_{k}':v for k,v in train(model, optim, train_loader, loss).items()}))\n",
    "    metrics.update(evaluate(model, optim, dev_loader, loss))\n",
    "    scheduler.step(metrics['loss'])\n",
    "    wandb.log(metrics)\n",
    "    if metrics['loss'] < best_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"state_dict.pth\"))\n",
    "        best_loss = metrics['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1000\n",
    "i = train_dataset[ix]\n",
    "images, masks = i['images'], i['masks']\n",
    "image = images['post']\n",
    "image = np.array(train_dataset.inverse_transform_image(image))\n",
    "\n",
    "util.vis_im_mask(image, masks['damage'], size=(512*2,512*2), opacity=.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(len(i['pre']['features']) for i in train_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0]/sum(counts.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
