{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.18 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "from xv import run\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from apex import amp\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from xv.nn.losses import loss_dict, WeightedLoss\n",
    "from pytorch_toolbelt import losses\n",
    "import pandas as pd\n",
    "from xv import dataset\n",
    "import random\n",
    "from xv.nn.layers import FrozenBatchNorm2d\n",
    "from xv.util import vis_im_mask\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "import os\n",
    "import wandb\n",
    "import yaml\n",
    "from xv import io\n",
    "from pprint import pprint\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "conf_file = \"config/config-damage.yaml\"\n",
    "# conf_file = \"config/config-seg-finetune.yaml\"\n",
    "# conf_file = \"config/config-seg-joint.yaml\"\n",
    "\n",
    "with open(conf_file) as f:\n",
    "    conf_init = yaml.load(f)\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'dryrun'\n",
    "wandb.init(project=conf_init['project'], config=conf_init, name=conf_init['name'])\n",
    "conf = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add_suppl': False,\n",
      " 'add_tertiary': False,\n",
      " 'amp_opt_level': 'O1',\n",
      " 'attention': 'scse',\n",
      " 'aug_prob': 0.25,\n",
      " 'batch_size': 6,\n",
      " 'class_weight': [1, 3, 2, 2],\n",
      " 'data_prefix': 'post',\n",
      " 'encoder': 'efficientnet-b3',\n",
      " 'epochs': 100,\n",
      " 'eval_resolution': 1024,\n",
      " 'filter_none': True,\n",
      " 'freeze_decoder_norm': False,\n",
      " 'freeze_encoder_norm': False,\n",
      " 'load_weights': False,\n",
      " 'loss_reduce_mode': 'mean',\n",
      " 'lr': 0.0002,\n",
      " 'metric': 'building:f1',\n",
      " 'mode': 'categorical',\n",
      " 'n_cpus': 10,\n",
      " 'name': '25rerun-lowlr',\n",
      " 'nclasses': 4,\n",
      " 'optim': 'adam',\n",
      " 'project': 'sky-eye-full',\n",
      " 'scheduler_factor': 0.1,\n",
      " 'scheduler_patience': 5,\n",
      " 'segmentation_arch': 'Unet',\n",
      " 'sync_bn': True,\n",
      " 'train_patch': False,\n",
      " 'train_repeat': 1,\n",
      " 'training_resolution': 1024,\n",
      " 'training_scales': [1.0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "from collections import OrderedDict\n",
    "\n",
    "backbone = EfficientNet.from_pretrained(conf.encoder)\n",
    "del backbone._fc\n",
    "\n",
    "preprocess_fn = smp.encoders.get_preprocessing_fn(conf.encoder)\n",
    "\n",
    "class DamageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, backbone, nclasses):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Conv2d(backbone._conv_head.out_channels, conf.nclasses, kernel_size=1)    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone.extract_features(x))\n",
    "\n",
    "model = DamageModel(backbone, conf.nclasses)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2519/2519 [00:14<00:00, 176.03it/s]\n",
      "100%|██████████| 280/280 [00:01<00:00, 190.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader = io.load_training_data(conf, preprocess_fn)\n",
    "dev_dataset, dev_loader = io.load_dev_data(conf, preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 2017\n",
      "n_dev: 224\n"
     ]
    }
   ],
   "source": [
    "print(f\"n_train: {len(train_dataset)}\")\n",
    "print(f\"n_dev: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/opt/anaconda3/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK2at11ATenOpTable11reportErrorEN3c1012TensorTypeIdE')\n"
     ]
    }
   ],
   "source": [
    "#loss = WeightedLoss({loss_dict[l](): w for l, w in conf.loss_weights.items()})\n",
    "\n",
    "weights = torch.Tensor(conf.class_weight).float().cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(weights, reduction=conf.loss_reduce_mode, ignore_index=-1)\n",
    "\n",
    "optims = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'sgd': torch.optim.SGD\n",
    "}\n",
    "\n",
    "optim = optims[conf.optim](model.parameters(), lr=conf.lr)\n",
    "\n",
    "\n",
    "model, optim = amp.initialize(model, optim, opt_level=conf.amp_opt_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def resize_heatmap(damage, damage_mask=None, scale=1/32):\n",
    "    if damage_mask is not None:\n",
    "        dmg_msk_dtype = damage_mask.dtype\n",
    "        damage_mask = misc_nn_ops.interpolate(damage_mask[None].float(), scale_factor=scale)[0].to(dmg_msk_dtype)\n",
    "    dmg_dtype = damage.dtype\n",
    "    damage_one_hot = torch.nn.functional.one_hot(damage).permute(0, 3, 1, 2)\n",
    "    damage = misc_nn_ops.interpolate(damage_one_hot.float(), scale_factor=scale).argmax(1)\n",
    "    return damage, damage_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, factor=conf.scheduler_factor, patience=conf.scheduler_patience\n",
    ")\n",
    "\n",
    "train_resize = run.MultiScaleResize(conf.mode, conf.training_scales)\n",
    "\n",
    "best_score = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, data, loss_fn, train_resize):\n",
    "    model = model.train()\n",
    "    loss_sum = 0.\n",
    "    for image, mask in tqdm(iter(data)):\n",
    "        if train_resize:\n",
    "            image, mask = train_resize((image, mask))\n",
    "        optim.zero_grad()\n",
    "        outputs = model(image.to('cuda'))\n",
    "        _, nclasses, _, _ = outputs.shape\n",
    "        mb, d_mask = mask\n",
    "        mb, d_mask = resize_heatmap(d_mask.cuda(),mb.cuda(), scale)\n",
    "        d_mask[mb] = -1\n",
    "        loss = loss_fn(outputs, d_mask)\n",
    "        with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optim.step()\n",
    "        loss_sum += loss\n",
    "    return {'train:loss':loss_sum.detach()/len(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [05:04<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = {'epoch': 0}\n",
    "train_metrics = train(model, optim, train_loader, loss_fn, train_resize=train_resize)\n",
    "metrics.update(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from xv.submission_metrics import RowPairCalculator\n",
    "from xv.run import get_metrics_for_counts\n",
    "import scipy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, loss_fn, threshold=0.5, nclasses=4):\n",
    "    model = model.eval()\n",
    "    metrics = {}\n",
    "    loss=0.\n",
    "    tps, fps, fns = defaultdict(float), defaultdict(float), defaultdict(float)\n",
    "    for image, mask in tqdm(iter(data)):\n",
    "        outputs = model(image.cuda())\n",
    "        mask_bool, d_mask = mask\n",
    "        \n",
    "        d_mask_down, mb_down = resize_heatmap(d_mask.cuda(), mask_bool.cuda(), scale)\n",
    "        d_mask_down[mb_down] = -1\n",
    "        loss = loss_fn(outputs, d_mask_down)\n",
    "        \n",
    "        output_big, _ = resize_heatmap(outputs.argmax(1), damage_mask=None, scale=1/scale)\n",
    "        output_big = output_big.float().cpu().numpy()\n",
    "        \n",
    "        flat_output, flat_target = output_big[mask_bool], d_mask[mask_bool].cpu().numpy()\n",
    "        \n",
    "        for ix in range(nclasses):                \n",
    "            tp, fn, fp = RowPairCalculator.compute_tp_fn_fp(flat_output, flat_target, ix)\n",
    "            tps[ix] += tp\n",
    "            fps[ix] += fp\n",
    "            fns[ix] += fn\n",
    "\n",
    "    metrics['loss'] = loss / len(data)\n",
    "    \n",
    "    aggregate = defaultdict(list)\n",
    "    for ix in range(nclasses):\n",
    "        categorical_ix_metrics =  get_metrics_for_counts(tps[ix], fps[ix], fns[ix])\n",
    "        for k,v in categorical_ix_metrics.items():\n",
    "            metrics[f'damage:categorical:{ix}:{k}'] = v\n",
    "            aggregate[f'damage:categorical:{k}'].append(v)\n",
    "    hmean = {f'hmean:{k}': scipy.stats.hmean(v) for k,v in aggregate.items()}\n",
    "    metrics.update(hmean)\n",
    "    \n",
    "    mean = {f'mean:{k}':scipy.mean(v) for k,v in aggregate.items()}\n",
    "    metrics.update(mean)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:13<00:00,  2.72it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Harmonic mean only defined if all elements greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3901b1aa6e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-919e449cc114>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data, loss_fn, threshold, nclasses)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'damage:categorical:{ix}:{k}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0maggregate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'damage:categorical:{k}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mhmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'hmean:{k}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-919e449cc114>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'damage:categorical:{ix}:{k}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0maggregate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'damage:categorical:{k}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mhmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mf'hmean:{k}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mhmean\u001b[0;34m(a, axis, dtype)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         raise ValueError(\"Harmonic mean only defined if all elements greater \"\n\u001b[0m\u001b[1;32m    399\u001b[0m                          \"than zero\")\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Harmonic mean only defined if all elements greater than zero"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m(398)\u001b[0;36mhmean\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    396 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    397 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 398 \u001b[0;31m        raise ValueError(\"Harmonic mean only defined if all elements greater \"\n",
      "\u001b[0m\u001b[0;32m    399 \u001b[0;31m                         \"than zero\")\n",
      "\u001b[0m\u001b[0;32m    400 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_loader, loss_fn, threshold=0.5, nclasses=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3f9f20bc01c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scheduler.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_resize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_resize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_fn' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, conf.epochs):\n",
    "    print(f\"epoch {epoch}/{conf.epochs}.\")\n",
    "    torch.save(optim.state_dict(), os.path.join(wandb.run.dir, \"optim.pth\"))\n",
    "    torch.save(scheduler.state_dict(), os.path.join(wandb.run.dir, \"scheduler.pth\"))\n",
    "    metrics = {'epoch': epoch}\n",
    "    train_metrics = train_fn(model, optim, train_loader, loss, train_resize=train_resize, mode=conf.mode)\n",
    "    metrics.update(train_metrics)\n",
    "\n",
    "    dev_metrics = eval_fn(model, dev_loader, loss, mode=conf.mode)\n",
    "    metrics.update(dev_metrics)\n",
    "    \n",
    "    \"\"\"\n",
    "    if conf.mode != \"dual\":\n",
    "        examples = run.sample_masks(model, dev_dataset.instances, preprocess_fn, n=1)\n",
    "        metrics['examples'] = [wandb.Image(im, caption=f'mask:{ix}') for e in examples for ix, im in enumerate(e)]\n",
    "    \"\"\"\n",
    "    \n",
    "    wandb.log(metrics)\n",
    "    #scheduler.step(metrics['loss'])\n",
    "    scheduler.step()\n",
    "    score = metrics[conf.metric]\n",
    "    pprint(metrics)\n",
    "    if score > best_score:\n",
    "        torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"state_dict.pth\"))\n",
    "        best_score = score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
