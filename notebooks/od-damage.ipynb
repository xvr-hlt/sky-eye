{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import wandb \n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "\n",
    "run_type = 'building-damage'\n",
    "conf_file = 'config/config-damage-od.yaml'\n",
    "\n",
    "wandb.init(project=run_type, config=yaml.load(open(conf_file)))\n",
    "conf = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from xv.util import vis_im_mask\n",
    "from pprint import pprint\n",
    "\n",
    "train_dir = '../../datasets/xview/train'\n",
    "suppl_dir = '../../datasets/xview/tier3'\n",
    "test_dir = '../../datasets/xview/test'\n",
    "\n",
    "pprint(dict(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as al\n",
    "from albumentations import BboxParams\n",
    "\n",
    "augment = al.Compose([\n",
    "    al.HorizontalFlip(p=conf.aug_prob),\n",
    "    al.VerticalFlip(p=conf.aug_prob),\n",
    "    al.RandomRotate90(p=conf.aug_prob),\n",
    "    al.Transpose(p=conf.aug_prob),\n",
    "    al.RandomBrightnessContrast(p=conf.aug_prob),\n",
    "    al.Rotate(p=conf.aug_prob),\n",
    "],  bbox_params=BboxParams('pascal_voc', label_fields = ['labels'], min_visibility=conf.min_bbox_visibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "from xv import dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_stems = pd.read_csv('config/train_stems.csv', header=None)[0]\n",
    "dev_stems = pd.read_csv('config/dev_stems.csv', header=None)[0]\n",
    "\n",
    "train_files = [f'{train_dir}/labels/{stem}_{conf.data_prefix}_disaster.json' for stem in train_stems]\n",
    "dev_files = [f'{train_dir}/labels/{stem}_{conf.data_prefix}_disaster.json' for stem in dev_stems]\n",
    "\n",
    "train_instances = dataset.get_instances(train_files, filter_none=conf.filter_none)\n",
    "dev_instances = dataset.get_instances(dev_files, filter_none=conf.filter_none)\n",
    "\n",
    "len(train_instances), len(dev_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.add_suppl:\n",
    "    train_instances *= conf.train_repeat\n",
    "    suppl_files = glob(f'{suppl_dir}/labels/*{conf.data_prefix}_disaster.json')\n",
    "    suppl_instances = dataset.get_instances(suppl_files, filter_none=conf.filter_none)\n",
    "    train_instances += suppl_instances\n",
    "    print(len(train_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.DamageClassificationDataset(\n",
    "    train_instances,\n",
    "    conf.nclasses,\n",
    "    augment=augment\n",
    ")\n",
    "\n",
    "dev_dataset = dataset.DamageClassificationDataset(\n",
    "    dev_instances,\n",
    "    conf.nclasses,\n",
    "    augment=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    include = [len(bx) > 0 for _, bx, _ in batch]\n",
    "    ims = torch.stack([torch.Tensor(ims) for ims, _, _ in batch])[include]\n",
    "    bxs, clss = [], []\n",
    "    for _, bx, cl in batch:\n",
    "        if len(bx) == 0:\n",
    "            continue\n",
    "        bxs.append(torch.Tensor(bx))\n",
    "        clss.append(torch.Tensor(cl))\n",
    "    return ims, bxs, clss\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from xv.nn.nets import BoxClassifier\n",
    "\n",
    "class MultiScaleResize(nn.Module):\n",
    "    def __init__(self, scales = (0.5, 0.75, 1.)):\n",
    "        super().__init__()\n",
    "        self.scales = scales\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, boxes):\n",
    "        scale = random.choice(self.scales)\n",
    "        if scale != 1.:\n",
    "            x = torch.nn.functional.interpolate(x, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "            boxes = [b*scale for b in boxes]\n",
    "        return x, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone, BackboneWithFPN, resnet, FeaturePyramidNetwork\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.backbone_norm = 'bn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = smp.encoders.encoders[conf.backbone]['out_shapes']\n",
    "# encoder = smp.encoders.get_encoder(conf.backbone, 'imagenet')\n",
    "# backbone = nn.Sequential(encoder, FeaturePyramidNetwork(in_channels, 256))\n",
    "# backbone.out_channels = 256\n",
    "# , featmap_names=[4,3,2,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "train_resize = MultiScaleResize(conf.training_scales)\n",
    "backbone = resnet_fpn_backbone('resnet101', True)\n",
    "model = BoxClassifier(backbone, conf.nclasses)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.pretrain_weights:\n",
    "    state_dict = torch.load(conf.pretrain_weights)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optims = {'adam': torch.optim.Adam}\n",
    "optim = optims[conf.optim](model.parameters(), lr=conf.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, factor=conf.scheduler_factor, patience=conf.scheduler_patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbelt import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbelt import losses\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from xv.nn.losses import loss_dict, WeightedLoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "\n",
    "#loss_fn = WeightedLoss({loss_dict[l]():w for l, w in conf.loss_weights.items()})\n",
    "\n",
    "#loss_fn = losses.JaccardLoss('multiclass')\n",
    "#loss_fn = CrossEntropyLoss(weights)\n",
    "\n",
    "if 'class_weight' in dict(conf):\n",
    "    weights = torch.Tensor(conf.class_weight).float().cuda()\n",
    "    loss_fn = CrossEntropyLoss(weights, reduction=conf.loss_reduce_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = losses.JointLoss(loss_fn, losses.FocalLoss(), 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pdb\n",
    "import logging\n",
    "\n",
    "def run(model, optim, data, train_resize, loss_fn):\n",
    "    model = model.train()\n",
    "    total_loss = 0.\n",
    "    for im, boxes, clss in tqdm(iter(data)):\n",
    "        if im.shape[0] == 0:\n",
    "            logging.warning(\"Empty batch.\")\n",
    "            continue\n",
    "        im, boxes = train_resize(im, boxes)\n",
    "        optim.zero_grad()\n",
    "        out = model(im.cuda(), [b.cuda() for b in boxes])\n",
    "        loss = loss_fn(out, torch.cat(clss).long().cuda())\n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "        optim.step()\n",
    "    return {'train_loss': total_loss/len(train_loader)}\n",
    "\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "from xv.run import get_metrics_for_counts\n",
    "\n",
    "def weighted_tp_fp_fn(pred, targ, weights, c):\n",
    "    tp = (np.logical_and(pred == c, targ == c) * weights).sum()\n",
    "    fp = (np.logical_and(pred != c, targ == c) * weights).sum()\n",
    "    fn = (np.logical_and(pred == c, targ != c) * weights).sum()\n",
    "    return tp, fp, fn\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, nclasses, loss_fn):\n",
    "    model.eval()\n",
    "    nclasses = conf.nclasses\n",
    "    loss_sum = 0.\n",
    "    tps, fps, fns = [0. for _ in range(nclasses)], [0. for _ in range(nclasses)], [0. for _ in range(nclasses)]\n",
    "    \n",
    "    for im, boxes, clss in tqdm(data):\n",
    "        res = im.shape[-1]\n",
    "        out = model(im.cuda(), [b.cuda() for b in boxes])\n",
    "        clss = torch.cat(clss).long()\n",
    "        loss_sum += loss_fn(out, clss.cuda())\n",
    "\n",
    "        out_ix = np.array(out.argmax(1).cpu())\n",
    "        clss = clss.cpu().numpy()\n",
    "        boxes_flat = torch.cat(boxes)\n",
    "        areas = (boxes_flat[:,2] - boxes_flat[:,0]) * (boxes_flat[:,3] - boxes_flat[:,1])\n",
    "        areas = areas.cpu().numpy()\n",
    "\n",
    "        for cl in range(nclasses):\n",
    "            tp, fp, fn = weighted_tp_fp_fn(out_ix, clss, areas, cl)\n",
    "            tps[cl] += tp\n",
    "            fps[cl] += fp\n",
    "            fns[cl] += fn\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['loss'] = loss_sum / len(data)\n",
    "    \n",
    "    aggregate = defaultdict(list)\n",
    "    for ix in range(nclasses):\n",
    "        categorical_ix_metrics =  get_metrics_for_counts(tps[ix], fps[ix], fns[ix])\n",
    "        for k,v in categorical_ix_metrics.items():\n",
    "            metrics[f'damage:categorical:{ix}:{k}'] = v\n",
    "            aggregate[f'damage:categorical:{k}'].append(v)\n",
    "\n",
    "    hmean = {f'hmean:{k}': scipy.stats.hmean(v) if all(v) else 0. for k,v in aggregate.items()}\n",
    "    metrics.update(hmean)\n",
    "\n",
    "    mean = {f'mean:{k}':scipy.mean(v) for k,v in aggregate.items()}\n",
    "    metrics.update(mean)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch, best_score = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch, conf.epochs):\n",
    "    metrics = {'epoch': epoch}\n",
    "    train_metrics = run(model, optim, train_loader, train_resize, loss_fn)\n",
    "    metrics.update(train_metrics)\n",
    "    \n",
    "    dev_metrics = evaluate(model, dev_loader, conf.nclasses, loss_fn)\n",
    "    metrics.update(dev_metrics)\n",
    "    \n",
    "    wandb.log(metrics)\n",
    "    scheduler.step(metrics['loss'])\n",
    "    score = metrics[conf.metric]\n",
    "    \n",
    "    if score > best_score:\n",
    "        torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"state_dict.pth\"))\n",
    "        best_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
