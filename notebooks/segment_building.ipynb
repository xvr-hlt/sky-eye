{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "import os\n",
    "import wandb\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#conf_file = \"config-damage.yaml\"\n",
    "conf_file = \"config-seg.yaml\"\n",
    "\n",
    "with open(conf_file) as f:\n",
    "    conf_init = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/xvr-hlt/sky-eye-full\" target=\"_blank\">https://app.wandb.ai/xvr-hlt/sky-eye-full</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/xvr-hlt/sky-eye-full/runs/sxjw9ees\" target=\"_blank\">https://app.wandb.ai/xvr-hlt/sky-eye-full/runs/sxjw9ees</a><br/>\n",
       "                Docs: <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">https://docs.wandb.com/integrations/jupyter.html</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.15 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "wandb.init(project=conf_init['project'], config=conf_init)\n",
    "conf = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add_suppl': False,\n",
      " 'amp_opt_level': 'O1',\n",
      " 'aug_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'data_prefix': 'pre',\n",
      " 'encoder': 'efficientnet-b7',\n",
      " 'epochs': 100,\n",
      " 'filter_none': False,\n",
      " 'freeze_decoder_norm': True,\n",
      " 'freeze_encoder_norm': True,\n",
      " 'load_weights': '../weights/state_dict.pth',\n",
      " 'loss_weights': {'focal': 4, 'jaccard': 1},\n",
      " 'lr': 5e-05,\n",
      " 'metric': 'building:f1',\n",
      " 'mode': None,\n",
      " 'nclasses': 1,\n",
      " 'optim': 'adam',\n",
      " 'project': 'sky-eye-full',\n",
      " 'scheduler_factor': 0.5,\n",
      " 'scheduler_patience': 5,\n",
      " 'segmentation_arch': 'Linknet',\n",
      " 'train_repeat': 1,\n",
      " 'training_resolution': 1024,\n",
      " 'training_scales': [0.75, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dict(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from xv.util import vis_im_mask\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../../datasets/xview/train'\n",
    "test_dir = '../../datasets/xview/test'\n",
    "suppl_dir = '../../datasets/xview/tier3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as al\n",
    "\n",
    "augment = al.Compose([\n",
    "        al.HorizontalFlip(p=conf.aug_prob),\n",
    "        al.VerticalFlip(p=conf.aug_prob),\n",
    "        al.RandomRotate90(p=conf.aug_prob),\n",
    "        al.Transpose(p=conf.aug_prob),\n",
    "        al.GridDistortion(p=conf.aug_prob, distort_limit=.2),\n",
    "        al.ShiftScaleRotate(p=conf.aug_prob),\n",
    "        al.RandomBrightnessContrast(p=conf.aug_prob),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn.nets import DownscaleLayer, XVNet\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "segmentation_types = {\n",
    "    'PSPNet': smp.PSPNet,\n",
    "    'FPN': smp.FPN,\n",
    "    'Linknet': smp.Linknet,\n",
    "    'Unet': smp.Unet\n",
    "}\n",
    "\n",
    "\n",
    "model_classes = conf.nclasses\n",
    "if conf.mode == \"ordinal\":\n",
    "    model_classes -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from xv.nn.layers import FrozenBatchNorm2d\n",
    "\n",
    "model = segmentation_types[conf.segmentation_arch](conf.encoder,\n",
    "                                                   classes=model_classes,\n",
    "                                                   activation='sigmoid')\n",
    "                                                   \n",
    "if conf.load_weights:\n",
    "    state_dict = torch.load(conf.load_weights)\n",
    "    print(model.load_state_dict(state_dict))\n",
    "\n",
    "preprocess_fn = get_preprocessing_fn(conf.encoder)\n",
    "\n",
    "if conf.freeze_encoder_norm:\n",
    "    model.encoder = FrozenBatchNorm2d.convert_frozen_batchnorm(model.encoder)\n",
    "    \n",
    "if conf.freeze_decoder_norm:\n",
    "    model.decoder = FrozenBatchNorm2d.convert_frozen_batchnorm(model.decoder)\n",
    "    \n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bd1e743e2d49bba23da1a5d0b279c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2240), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559521a51d4a45dfa04be9b090a10adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=559), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2240, 559)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from xv import dataset\n",
    "\n",
    "random.seed(hash(\"ðŸ˜‚\"))\n",
    "\n",
    "\n",
    "all_files = glob(f'{train_dir}/labels/*{conf.data_prefix}_disaster.json')\n",
    "random.shuffle(all_files)\n",
    "\n",
    "dev_ix = int(len(all_files)*.20)\n",
    "dev_files = all_files[:dev_ix]\n",
    "train_files = all_files[dev_ix:]\n",
    "\n",
    "train_instances = dataset.get_instances(train_files, filter_none=conf.filter_none)\n",
    "\n",
    "dev_instances = dataset.get_instances(dev_files, filter_none=conf.filter_none)\n",
    "\n",
    "len(train_instances), len(dev_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.add_suppl:\n",
    "    conf.train_repeat = 1\n",
    "    train_instances *= conf.train_repeat\n",
    "    suppl_files = glob(f'{suppl_dir}/labels/*{conf.data_prefix}_disaster.json')\n",
    "    suppl_instances = dataset.get_instances(suppl_files, filter_none=conf.filter_none)\n",
    "    train_instances += suppl_instances\n",
    "    print(len(train_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.BuildingSegmentationDataset(\n",
    "    instances=train_instances,\n",
    "    nclasses=conf.nclasses,\n",
    "    resolution=conf.training_resolution,\n",
    "    augment=augment,\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    mode=conf.mode,\n",
    ")\n",
    "\n",
    "dev_dataset = dataset.BuildingSegmentationDataset(\n",
    "    instances=dev_instances,\n",
    "    nclasses=conf.nclasses,\n",
    "    resolution=conf.training_resolution,\n",
    "    augment=None,\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    mode=conf.mode,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    ")\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn.losses import loss_dict, WeightedLoss\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "if 'class_weight' in dict(conf):\n",
    "    weights = torch.Tensor(conf.class_weight).float().cuda()\n",
    "    loss = CrossEntropyLoss(weights)\n",
    "else:\n",
    "    loss = WeightedLoss({loss_dict[l]():w for l, w in conf.loss_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightedLoss()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex\n",
    "\n",
    "optims = {\n",
    "    'adam': torch.optim.Adam\n",
    "}\n",
    "\n",
    "optim = optims[conf.optim](model.parameters(), lr=conf.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "model, optim = amp.initialize(model, optim, opt_level=conf.amp_opt_level);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, factor=conf.scheduler_factor, patience=conf.scheduler_patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import misc as misc_nn_ops\n",
    "import random\n",
    "\n",
    "class MultiScaleResize(nn.Module):\n",
    "    def __init__(self, mode=\"categorical\", scales = (0.5, 0.75, 1.)):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.scales = scales\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, batch):\n",
    "        scale = random.choice(self.scales)\n",
    "        if scale == 1.:\n",
    "            return batch\n",
    "        if self.mode is None:\n",
    "            im, mask = batch\n",
    "            mask_dtype = mask.dtype\n",
    "            im = torch.nn.functional.interpolate(im, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "            mask = misc_nn_ops.interpolate(mask.float(), scale_factor=scale).to(mask_dtype)\n",
    "            return im, mask\n",
    "        if self.mode == \"categorical\":\n",
    "            im, (damage_mask, damage) = batch\n",
    "            dmg_msk_dtype = damage_mask.dtype()\n",
    "            dmg_dtype = damage.dtype()\n",
    "            im = torch.nn.functional.interpolate(im, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "            damage_mask = misc_nn_ops.interpolate(damage_mask[None].float(), scale_factor=scale)[0].to(dmg_msk_dtype)\n",
    "            damage_one_hot = torch.nn.functional.one_hot(damage).permute(0,3,1,2)\n",
    "            damage = misc_nn_ops.interpolate(damage_one_hot.float(), scale_factor=scale).argmax(1)\n",
    "            return im, (damage_mask, damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resize = MultiScaleResize(conf.mode, conf.training_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf148f7001249e3855f14b10049f91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2240), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xv import run\n",
    "\n",
    "train_fn = run.train_segment if conf.nclasses == 1 else run.train_damage\n",
    "eval_fn = run.evaluate_segment if conf.nclasses == 1 else run.evaluate_damage\n",
    "\n",
    "for epoch in range(epoch, conf.epochs):\n",
    "    metrics = {'epoch': epoch}\n",
    "    train_metrics = train_fn(model, optim, train_loader, loss, train_resize=train_resize, mode=conf.mode)\n",
    "    metrics.update(train_metrics)\n",
    "    \n",
    "    dev_metrics = eval_fn(model, dev_loader, loss, mode=conf.mode)\n",
    "    metrics.update(dev_metrics)\n",
    "    \n",
    "    examples = run.sample_masks(model, dev_instances, preprocess_fn, n=3)\n",
    "    metrics['examples'] = [wandb.Image(im, caption=f'mask:{ix}') for e in examples for ix, im in enumerate(e)]\n",
    "    \n",
    "    wandb.log(metrics)\n",
    "    scheduler.step(metrics['loss'])\n",
    "    score = metrics[conf.metric]\n",
    "\n",
    "    if score > best_score:\n",
    "        torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"state_dict.pth\"))\n",
    "        best_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_metrics = eval_fn(model, dev_loader, loss, mode=conf.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.0622, device='cuda:0'),\n",
       " 'building:precision': tensor(0.8562, device='cuda:0'),\n",
       " 'building:recall': tensor(0.8782, device='cuda:0'),\n",
       " 'building:f1': tensor(0.8671, device='cuda:0')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1000\n",
    "i = train_dataset[ix]\n",
    "images, masks = i['images'], i['masks']\n",
    "image = images['post']\n",
    "image = np.array(train_dataset.inverse_transform_image(image))\n",
    "util.vis_im_mask(image, masks['damage'], size=(512*2, 512*2), opacity=.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(len(i['pre']['features']) for i in train_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in run.sample_masks(model, dev_instances, preprocess_fn, sz=1024):\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for i in random.sample(dev_instances, 1):\n",
    "    with torch.no_grad():\n",
    "        img = np.array(Image.open(i['file_name']))\n",
    "        model_in = preprocess_fn(img).transpose(2,0,1)\n",
    "        model_in = torch.tensor(model_in)\n",
    "        model_in = model_in.reshape(1, *model_in.shape)\n",
    "        mask = model(model_in.cuda())\n",
    "        mask = np.array((mask > 0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv import run\n",
    "tps, fps, fns = [], [], []\n",
    "model = model.eval()\n",
    "threshold=0.5\n",
    "with torch.no_grad():\n",
    "    for image, mask in tqdm(iter(dev_loader)):\n",
    "        out = model(image.to('cuda'))\n",
    "        for o, m in zip(out, mask):\n",
    "            tp, fp, fn = run.get_tp_fp_fn(o, m, threshold)\n",
    "            tps.append(np.array(torch.tensor(tp).cpu()))\n",
    "            fps.append(np.array(torch.tensor(fp).cpu()))\n",
    "            fns.append(np.array(torch.tensor(fn).cpu()))\n",
    "            \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tp': tps, 'fp': fps, 'fn':fns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df.fp + df.fn).sort_values(ascending=False).index\n",
    "df.iloc[idx].head()\n",
    "\n",
    "from PIL import Image\n",
    "from xv import util\n",
    "import cv2\n",
    "\n",
    "\n",
    "i = dev_instances[ix]\n",
    "sz = conf.training_resolution\n",
    "\n",
    "#def analyse_instance(i):\n",
    "with torch.no_grad():\n",
    "    img = np.array(Image.open(i['file_name']))\n",
    "    img = cv2.resize(img, (sz,sz))\n",
    "    model_in = preprocess_fn(img).transpose(2,0,1)\n",
    "    model_in = torch.tensor(model_in).float()\n",
    "    model_in = model_in.reshape(1, *model_in.shape)\n",
    "    mask = model(model_in.cuda())\n",
    "    mask = np.array((mask > 0).cpu())\n",
    "    im = util.vis_im_mask(img, mask[0], opacity=.3, size=(sz,sz))\n",
    "    \n",
    "_, target = dev_dataset[ix]\n",
    "mask = mask.astype(bool).reshape(sz,sz)\n",
    "target = target.astype(bool).reshape(sz,sz)\n",
    "\n",
    "tp = target & mask\n",
    "fp = mask & ~target\n",
    "fn = target & ~mask\n",
    "\n",
    "util.vis_im_mask(img, target, opacity=.2, colours=(\"blue\", \"yellow\", \"red\"))\n",
    "util.vis_im_mask(img, np.stack((tp, fn, fp)), opacity=.2, colours=(\"blue\", \"yellow\", \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_model = tta.SegmentationTTAWrapper(model, transforms, merge_mode='mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
