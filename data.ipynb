{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'data.ipynb'\n",
    "wandb.init(\"sky-eye-full\")\n",
    "conf = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dict(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "train_dir = '/home/jovyan/work/datasets/xview/train'\n",
    "test_dir = '/home/jovyan/work/datasets/xview/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from xv.util import vis_im_mask\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as al\n",
    "\n",
    "augment = al.Compose([\n",
    "        al.HorizontalFlip(p=conf.aug_prob),\n",
    "        al.VerticalFlip(p=conf.aug_prob),\n",
    "        al.RandomRotate90(p=conf.aug_prob),\n",
    "        al.Transpose(p=conf.aug_prob),\n",
    "        al.GridDistortion(p=conf.aug_prob, distort_limit=.2),\n",
    "        al.ShiftScaleRotate(p=conf.aug_prob),\n",
    "        al.RandomBrightnessContrast(p=conf.aug_prob)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn.solaris.model_io import get_model\n",
    "from xv.nn.nets import DownscaleLayer, XVNet\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "block_types = {\n",
    "    'bottleneck': Bottleneck,\n",
    "    'basic': BasicBlock\n",
    "}\n",
    "\n",
    "segmentation_types = {\n",
    "    'PSPNet': smp.PSPNet,\n",
    "    'FPN': smp.FPN,\n",
    "    'Linknet': smp.Linknet,\n",
    "    'Unet': smp.Unet\n",
    "}\n",
    "\n",
    "if 'pretrained_model' in dict(conf):\n",
    "    building_seg = get_model(conf.pretrained_model, 'torch', pretrained=conf.pretrained)\n",
    "    preprocess_fn=None\n",
    "else:\n",
    "    building_seg = segmentation_types[conf.segmentation_arch](conf.encoder, classes=1, activation='sigmoid')\n",
    "    preprocess_fn = get_preprocessing_fn(conf.encoder)\n",
    "\n",
    "if conf.train_post:\n",
    "    damage = DownscaleLayer(inplanes=building_seg.final_filters, blocks=conf.blocks, strides=conf.strides,\n",
    "                           block=block_types[conf.blocktype],\n",
    "                           nclasses=conf.n_dmg_classes,\n",
    "                           growth_rate=conf.growth_rate)\n",
    "else:\n",
    "    damage = None\n",
    "\n",
    "model = XVNet(building_seg, damage).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn import dataset\n",
    "from xv import util\n",
    "import random\n",
    "\n",
    "dmg_downscale=1\n",
    "for s in conf.strides:\n",
    "    dmg_downscale *= s\n",
    "    \n",
    "dmg_downscale_ratio = dmg_downscale\n",
    "\n",
    "\n",
    "\n",
    "instances = dataset.XViewSegmentationDataset.get_instances(train_dir)\n",
    "\n",
    "random.seed(hash(\"ðŸ˜‚\"))\n",
    "random.shuffle(instances)\n",
    "\n",
    "dev_ix = int(len(instances)*.20)\n",
    "dev_instances = instances[:dev_ix]\n",
    "train_instances = instances[dev_ix:]\n",
    "len(train_instances), len(dev_instances)\n",
    "\n",
    "train_dataset = dataset.XViewSegmentationDataset(\n",
    "    instances=train_instances,\n",
    "    resolution=(conf.image_size, conf.image_size),\n",
    "    dmg_downscale_ratio = dmg_downscale_ratio,\n",
    "    augment=augment,\n",
    "    damage_scale_mode=conf.damage_scale_mode,\n",
    "    preprocess_fn=preprocess_fn,\n",
    ")\n",
    "\n",
    "dev_dataset = dataset.XViewSegmentationDataset(\n",
    "    instances=dev_instances,\n",
    "    resolution=(conf.image_size, conf.image_size),\n",
    "    dmg_downscale_ratio = dmg_downscale_ratio,\n",
    "    augment=None,\n",
    "    damage_scale_mode=conf.damage_scale_mode,\n",
    "    preprocess_fn=preprocess_fn\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=conf.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv.nn.losses import loss_dict, WeightedLoss\n",
    "loss = WeightedLoss({loss_dict[l]():w for l, w in conf.loss_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex\n",
    "\n",
    "optims = {\n",
    "    'adam': torch.optim.Adam\n",
    "}\n",
    "optim = optims[conf.optim](model.parameters(), lr=conf.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "model, optim = amp.initialize(model, optim, opt_level=conf.amp_opt_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=conf.scheduler_factor, patience=conf.scheduler_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_weight = conf.pre_weight if conf.train_pre else None\n",
    "post_weight = conf.post_weight if conf.train_post else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e5\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xv import run\n",
    "for epoch in range(epoch, conf.epochs):\n",
    "    metrics = {'epoch':epoch}\n",
    "    metrics.update((\n",
    "        {f'train_{k}':v for k,v in\n",
    "         run.train(model, optim, train_loader, loss, pre_weight=pre_weight, post_weight=post_weight).items()}\n",
    "    ))\n",
    "    metrics.update(\n",
    "        run.evaluate(model, dev_loader, loss, pre_weight=pre_weight, post_weight=post_weight\n",
    "    ))\n",
    "    wandb.log(metrics)\n",
    "    scheduler.step(metrics['loss'])\n",
    "    if metrics['loss'] < best_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"state_dict.pth\"))\n",
    "        best_loss = metrics['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1000\n",
    "i = train_dataset[ix]\n",
    "images, masks = i['images'], i['masks']\n",
    "image = images['post']\n",
    "image = np.array(train_dataset.inverse_transform_image(image))\n",
    "\n",
    "util.vis_im_mask(image, masks['damage'], size=(512*2,512*2), opacity=.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(len(i['pre']['features']) for i in train_dataset.instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
